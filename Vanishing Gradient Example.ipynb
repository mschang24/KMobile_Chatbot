{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradient Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple fully connected neural network to classify MNIST images. The gradients of the weights at each timestep are recorded and plotted to visualize how they change over time and how they compare to each other.  Try changing the number or size of layers to see how that affects the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries and data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll train on a small subset of the training data of size 5000, but use this whole subset for training at each step.\n",
    "# We do this to get a smoother graph of gradients.  SGD or mini-batch gradient descent is a lot noisier.\n",
    "# Here, we get a stratified sample of the training set to make sure we have about equal number of each label in the \n",
    "# subset we'll be using.\n",
    "label_numbers = [np.where(x==1)[0][0] for x in mnist.train.labels]\n",
    "sss = StratifiedShuffleSplit(y=label_numbers, n_iter=1, test_size=5000, random_state=0)\n",
    "   \n",
    "for large_split_indices, small_split_indices in sss:\n",
    "    small_split_data = mnist.train.images[small_split_indices]\n",
    "    small_split_labels = mnist.train.labels[small_split_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define our train, validation and testing sets\n",
    "train_dataset = small_split_data\n",
    "train_labels = small_split_labels\n",
    "valid_dataset = mnist.validation.images\n",
    "valid_labels = mnist.validation.labels\n",
    "test_dataset = mnist.test.images\n",
    "test_labels = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll be using all of the small training subset at each step, to get smoother gradients.\n",
    "train_subset = len(train_dataset)\n",
    "\n",
    "# This is our lambda parameter for regularization.\n",
    "y = .01\n",
    "\n",
    "image_size = 28 \n",
    "num_labels = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    tf_train_dataset = tf.constant(train_dataset)\n",
    "    tf_train_labels = tf.cast(tf.constant(train_labels), tf.float32)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Initialize the weights and biases, the parameters we'll be training.\n",
    "    num_layers = 3\n",
    "    weights_array = []\n",
    "    biases_array = []\n",
    "    layer_sizes = [image_size*image_size, 30, 30, num_labels]\n",
    "    for i in range(num_layers):\n",
    "        weights_array.append(tf.get_variable(('weight'+str(i)), shape=[layer_sizes[i], layer_sizes[i+1]],\n",
    "               initializer=tf.contrib.layers.xavier_initializer()))\n",
    "        biases_array.append(tf.Variable(tf.zeros([layer_sizes[i+1]])))\n",
    "  \n",
    "\n",
    "    # Train the network by sequentially multiplying inputs to weight matrices, adding biases, and taking\n",
    "    # the sigmoid of the output.  We compute the softmax probabilities out of the last layer, and use the \n",
    "    # average cross-entropy across all samples plus regularization as our loss.\n",
    "    logits = tf.nn.sigmoid(tf.matmul(tf_train_dataset, weights_array[0]) + biases_array[0])\n",
    "    for i in range(1,num_layers-1):\n",
    "        logits = tf.nn.sigmoid(tf.matmul(logits, weights_array[i]) + biases_array[i])\n",
    "    logits = tf.matmul(logits, weights_array[num_layers-1]) + biases_array[num_layers-1]\n",
    "\n",
    "    l2 = 0  # regularization term\n",
    "    for i in range(num_layers):\n",
    "        l2 += tf.nn.l2_loss(weights_array[i]) \n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + y*l2\n",
    "  \n",
    "    # Use gradient descent to calculate the gradients with respect to our parameters.\n",
    "    # This is how we'll minimize the loss.\n",
    "    opt = tf.train.GradientDescentOptimizer(0.5)\n",
    "    grads_and_vars = opt.compute_gradients(loss)\n",
    "    apply_grads = opt.minimize(loss)\n",
    "\n",
    "    # Predictions \n",
    "\n",
    "    # Train\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Validation \n",
    "    logits = tf.nn.sigmoid(tf.matmul(tf_valid_dataset, weights_array[0]) + biases_array[0])\n",
    "    for i in range(1,num_layers-1):\n",
    "        logits = tf.nn.sigmoid(tf.matmul(logits, weights_array[i]) + biases_array[i])\n",
    "    logits = tf.matmul(logits, weights_array[num_layers-1]) + biases_array[num_layers-1]\n",
    "\n",
    "    valid_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Test\n",
    "    logits = tf.nn.sigmoid(tf.matmul(tf_test_dataset, weights_array[0]) + biases_array[0])\n",
    "    for i in range(1,num_layers-1):\n",
    "        logits = tf.nn.sigmoid(tf.matmul(logits, weights_array[i]) + biases_array[i])\n",
    "    logits = tf.matmul(logits, weights_array[num_layers-1]) + biases_array[num_layers-1]\n",
    "\n",
    "    test_prediction = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grads_and_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # One time operation to initialize the graph\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    print('Initialized')\n",
    "\n",
    "    # Initialize the dictionary we'll use to store the gradients\n",
    "    var_to_grad = {}\n",
    "\n",
    "    for step in range(num_steps):\n",
    "\n",
    "        grad_vals, _, l, predictions = session.run([[grad for grad,_ in grads_and_vars], apply_grads, loss, train_prediction])\n",
    "    \n",
    "        # Add the gradients from each step to our dictionary\n",
    "        for grad_val, (_, var) in zip(grad_vals, grads_and_vars):\n",
    "            if var.name in var_to_grad:\n",
    "                var_to_grad[var.name].append(np.mean(np.abs(grad_val)))\n",
    "            else: \n",
    "                var_to_grad[var.name] = [np.mean(np.abs(grad_val))]\n",
    "\n",
    "        if (step % 100 == 0):  \n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(\n",
    "                predictions, train_labels[:train_subset, :]))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for l in range(num_layers):\n",
    "    gradients = var_to_grad['weight'+str(l)+\":0\"]\n",
    "    plt.plot(gradients, label=\"layer \"+str(l))\n",
    "ax = plt.axes() \n",
    "ax.set_ylim((0,.005))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
